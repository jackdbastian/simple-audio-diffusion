{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import UNet2DModel, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator params\n",
    "GRADIENT_ACCUMULATION_STEPS=1\n",
    "MIXED_PRECISION='no'\n",
    "LOGGING_DIR='logs'\n",
    "\n",
    "# dataset params\n",
    "DATA_PATH='data/audio-diffusion-256'\n",
    "BATCH_SIZE=10\n",
    "NUM_TRAIN_STEPS=10\n",
    "\n",
    "# optimizer params\n",
    "LEARNING_RATE=1e-4\n",
    "ADAM_BETA1=0.95\n",
    "ADAM_BETA2=0.999\n",
    "ADAM_WEIGHT_DECAY=1e-6\n",
    "ADAM_EPSILON=1e-08\n",
    "\n",
    "# lr_scheduler params\n",
    "LR_SCHEDULER_TYPE='cosine'\n",
    "LR_WARMUP_STEPS=50\n",
    "NUM_EPOCHS=10\n",
    "\n",
    "# EMAModel Params\n",
    "USE_EMA=True\n",
    "EMA_INV_GAMMA=1.0\n",
    "EMA_POWER=3 /4\n",
    "EMA_MAX_DECAY=0.99999\n",
    "\n",
    "# training params\n",
    "START_EPOCH=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    mixed_precision=MIXED_PRECISION,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_dir=LOGGING_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "dataset = load_from_disk(DATA_PATH)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# plot a few spectrograms\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset[:\u001b[39m4\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mimages\u001b[39;49m\u001b[39m\"\u001b[39;49m]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     axs[i]\u001b[39m.\u001b[39mimshow(image)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     axs[i]\u001b[39m.\u001b[39mset_axis_off()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'images'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a few spectrograms\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, image in enumerate(dataset[:4][\"images\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = dataset[0][\"image\"].height, dataset[0][\"image\"].width\n",
    "\n",
    "augmentations = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5]), # rescale pixels to b/t -1 and 1\n",
    "])\n",
    "\n",
    "def transforms(examples):\n",
    "    images = [augmentations(image) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "# apply the augmentations() func on the fly during training\n",
    "dataset.set_transform(transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=resolution,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/diffusers/training_utils.py:73: FutureWarning: Passing a `torch.nn.Module` to `ExponentialMovingAverage` is deprecated. Please pass the parameters of the module instead.\n",
      "  deprecate(\n",
      "/Users/jack/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/diffusers/training_utils.py:86: FutureWarning: The `max_value` argument is deprecated. Please use `decay` instead.\n",
      "  deprecate(\"max_value\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "# define scheduler\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=NUM_TRAIN_STEPS)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    "    eps=ADAM_EPSILON,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    LR_SCHEDULER_TYPE,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=LR_WARMUP_STEPS,\n",
    "    num_training_steps=(len(train_dataloader) * NUM_EPOCHS) //\n",
    "    GRADIENT_ACCUMULATION_STEPS,\n",
    ")\n",
    "\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "\n",
    "ema_model = EMAModel(\n",
    "    getattr(model, \"module\", model),\n",
    "    inv_gamma=EMA_INV_GAMMA,\n",
    "    power=EMA_POWER,\n",
    "    max_value=EMA_MAX_DECAY,\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    run = 'train'\n",
    "    accelerator.init_trackers(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 32.10 GB, other allocations: 3.91 GB, max allowed: 36.27 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m noisy_images \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39madd_noise(clean_images, noise, timesteps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mwith\u001b[39;00m accelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# Predict the noise residual\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     noise_pred \u001b[39m=\u001b[39m model(noisy_images, timesteps)[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jack/Documents/python/simple-audio-diffusion/tests.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     accelerator\u001b[39m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/diffusers/models/unet_2d.py:317\u001b[0m, in \u001b[0;36mUNet2DModel.forward\u001b[0;34m(self, sample, timestep, class_labels, return_dict)\u001b[0m\n\u001b[1;32m    315\u001b[0m         sample, skip_sample \u001b[39m=\u001b[39m upsample_block(sample, res_samples, emb, skip_sample)\n\u001b[1;32m    316\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         sample \u001b[39m=\u001b[39m upsample_block(sample, res_samples, emb)\n\u001b[1;32m    319\u001b[0m \u001b[39m# 6. post-process\u001b[39;00m\n\u001b[1;32m    320\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_norm_out(sample)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/diffusers/models/unet_2d_blocks.py:2316\u001b[0m, in \u001b[0;36mUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, scale)\u001b[0m\n\u001b[1;32m   2312\u001b[0m             hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   2313\u001b[0m                 create_custom_forward(resnet), hidden_states, temb\n\u001b[1;32m   2314\u001b[0m             )\n\u001b[1;32m   2315\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2316\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb, scale\u001b[39m=\u001b[39;49mscale)\n\u001b[1;32m   2318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2319\u001b[0m     \u001b[39mfor\u001b[39;00m upsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/diffusers/models/resnet.py:650\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, scale)\u001b[0m\n\u001b[1;32m    648\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states, temb)\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(hidden_states)\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding_norm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscale_shift\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    653\u001b[0m     scale, shift \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(temb, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/modules/normalization.py:279\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgroup_norm(\n\u001b[1;32m    280\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_groups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/nn/functional.py:2558\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2557\u001b[0m _verify_batch_size([\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups, num_groups] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:]))\n\u001b[0;32m-> 2558\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgroup_norm(\u001b[39minput\u001b[39;49m, num_groups, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "File \u001b[0;32m~/.virtualenvs/simple-audio-diffusion/lib/python3.11/site-packages/torch/_refs/__init__.py:3044\u001b[0m, in \u001b[0;36mnative_group_norm\u001b[0;34m(input, weight, bias, batch_size, num_channels, flattened_inner_size, num_groups, eps)\u001b[0m\n\u001b[1;32m   3041\u001b[0m     unsqueeze_weight \u001b[39m=\u001b[39m _unsqueeze_multiple(weight, broadcast_dims)\n\u001b[1;32m   3043\u001b[0m \u001b[39mif\u001b[39;00m unsqueeze_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3044\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m*\u001b[39;49m unsqueeze_weight\n\u001b[1;32m   3045\u001b[0m \u001b[39mif\u001b[39;00m unsqueeze_bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3046\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m unsqueeze_bias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 32.10 GB, other allocations: 3.91 GB, max allowed: 36.27 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# training\n",
    "global_step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm(total=len(train_dataloader),\n",
    "                        disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "    if epoch < START_EPOCH:\n",
    "        for step in range(len(train_dataloader)):\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "        if epoch == START_EPOCH - 1 and USE_EMA:\n",
    "            ema_model.optimization_step = global_step\n",
    "        continue\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"input\"]\n",
    "\n",
    "        # Sample noise that we'll add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bsz = clean_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz, ),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            if USE_EMA:\n",
    "                ema_model.step(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "        }\n",
    "        if USE_EMA:\n",
    "            logs[\"ema_decay\"] = ema_model.decay\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "    progress_bar.close()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-audio-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
